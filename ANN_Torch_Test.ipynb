{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN_Torch_Test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwCrsIAULUYQ"
      },
      "source": [
        "My goal here is to implement an ANN using pytorch. There's a lot of tutorials using NMIST, and I had a lot of guidance from a tutorial created by the youtube channel \"Sentdex.\" In particular, I used the text tutorial given here\n",
        "https://pythonprogramming.net/data-deep-learning-neural-network-pytorch/ to start me off.\n",
        "\n",
        "Once I had an idea of how to use torch, I tried to do this on my own with a toy dataset like digits without any batching to see if I understood everything, and then I implemented a batching proceedure to speed this up. I want to try and implement a parallel type code for batching, but this would be for later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTTZAz0e93LA"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeT9P45vJ3K9"
      },
      "source": [
        "# train = datasets.MNIST('', train=True, download=True,\n",
        "#                        transform=transforms.Compose([\n",
        "#                            transforms.ToTensor()\n",
        "#                        ]))\n",
        "\n",
        "# test = datasets.MNIST('', train=False, download=True,\n",
        "#                        transform=transforms.Compose([\n",
        "#                            transforms.ToTensor()\n",
        "#                        ]))\n",
        "# trainset = torch.utils.data.DataLoader(train, shuffle=True)\n",
        "# testset = torch.utils.data.DataLoader(test, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FKM3gJcDb9h",
        "outputId": "1f86d808-eb74-47b9-9fb0-3e88b0bd5478",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X, y = datasets.load_digits(return_X_y = True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "print (X_train.shape)\n",
        "scaler = StandardScaler() \n",
        "scaler.fit(X_train)   # calculate mean\n",
        "X_train_norm = scaler.transform(X_train)  # apply normalization on X_train\n",
        "X_test_norm = scaler.transform(X_test)    # apply normalization on X_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1347, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvJX5ijYESk7"
      },
      "source": [
        "X_train_tensor = torch.from_numpy(X_train_norm)\n",
        "X_test_tensor = torch.from_numpy(X_test_norm)\n",
        "y_train_tensor = torch.from_numpy(y_train)\n",
        "y_test_tensor = torch.from_numpy(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjqamWVyLSwY"
      },
      "source": [
        "def accuracy(ypred, yexact):\n",
        "    p = np.array(ypred == yexact, dtype=int)\n",
        "    return np.sum(p) / float(len(yexact))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1QFUSu9-2ui"
      },
      "source": [
        "class ANN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ANN, self).__init__()\n",
        "        self.Linear1 = nn.Linear(64, 32)\n",
        "        self.Linear2 = nn.Linear(32, 32)\n",
        "        self.Linear3 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(x.shape)\n",
        "        x = F.relu(self.Linear1(x))\n",
        "        x = F.relu(self.Linear2(x))\n",
        "        x = self.Linear3(x)\n",
        "        return F.log_softmax(x, dim = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s64jjbuB_DA5",
        "outputId": "10721015-3d55-43a9-dd8e-89e480b53229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model = ANN()\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "for epoch in range(1000): \n",
        "    model.zero_grad() \n",
        "    output = model(X_train_tensor.float())  \n",
        "    loss = F.nll_loss(output, y_train_tensor) \n",
        "    loss.backward()  \n",
        "    optimizer.step()  \n",
        "print(torch.argmax(output, axis = 1))\n",
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1, 1, 8,  ..., 2, 7, 1])\n",
            "[1 1 8 ... 2 7 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8td3o4_3YRaH",
        "outputId": "6682a841-616f-4248-bda9-652548c605f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with torch.no_grad():\n",
        "    output = model(X_test_tensor.float())\n",
        "    out_np = output.detach().numpy()\n",
        "predict = np.argmax(out_np, axis = 1)\n",
        "acc = accuracy(predict, y_test)\n",
        "print(acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9747474747474747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i56MEvxNM4nP"
      },
      "source": [
        "Okay, it seems like I have a grasp of how to handle everything now. I'll try to write some code to batch everything now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_75xkHLNVi_",
        "outputId": "fc2cd8cb-dc3d-486b-df82-360d131e4b9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = ANN()\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "max_epochs = 3\n",
        "batch_size = 50\n",
        "num_datapoints = X_train.shape[0]\n",
        "\n",
        "for epoch in range(50):\n",
        "    for i in range(batch_size):\n",
        "        if((i+1)*batch_size < num_datapoints):\n",
        "            X_batch = X_train_tensor[i*batch_size:(i+1)*batch_size,:]\n",
        "            y_batch = y_train_tensor[i*batch_size:(i+1)*batch_size]\n",
        "        else:\n",
        "            X_batch = X_train_tensor[i*batch_size:num_datapoints,:]\n",
        "            y_batch = y_train_tensor[i*batch_size:num_datapoints]\n",
        "        model.zero_grad() \n",
        "        output = model(X_batch.float())  \n",
        "        loss = F.nll_loss(output, y_batch) \n",
        "        loss.backward()  \n",
        "        optimizer.step()\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(X_test_tensor.float())\n",
        "    out_np = output.detach().numpy()\n",
        "predict = np.argmax(out_np, axis = 1)\n",
        "print(predict)\n",
        "print(y_test)\n",
        "            \n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6 9 3 7 2 2 5 2 5 2 1 8 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9\n",
            " 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 9 5 4\n",
            " 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7\n",
            " 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 9 0 6 6 4 2 8 0 9 4 6 9 9 6 9\n",
            " 0 5 5 6 6 0 6 4 2 9 3 8 7 2 9 0 4 5 3 6 5 9 9 8 4 2 1 3 7 7 2 2 3 9 8 0 3\n",
            " 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 1 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7\n",
            " 4 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 5 8 5 5 1 0 2 8 8 9 9 7 6 2 2 2 3 4 8 8\n",
            " 3 6 0 9 7 7 0 1 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5\n",
            " 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8\n",
            " 4 5 8 7 9 8 6 0 6 2 0 7 9 1 9 5 2 7 7 1 8 7 4 3 8 3 5 6 0 0 3 0 5 0 0 4 1\n",
            " 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 7 4 8 3 4\n",
            " 0 5 1 9 4 5 7 6 3 7 0 5 9 7 5 9 7 4 2 1 9 0 7 5 2 3 6 3 9 6 9 5 0 1 5 5 8\n",
            " 3 3 6 2 6 5 6 2 0 8 7 3 7 0 2 2 3 5 8 7 3 6 5 9 9 2 9 6 3 0 7 1 1 9 6 1 1\n",
            " 0 0 2 9 3 9 9 3 7 7 1 3 5 4 6 1 2 1 1 8 7 6 9 2 0 4 4 8 8 7 1 3 1 7 1 8 5\n",
            " 1 7 0 0 2 2 6 9 4 1 9 0 6 7 7 9 5 4 7 0 7 6 8 7 1 4 6 2 8 7 5 9 0 3 9 6 6\n",
            " 1 9 1 2 9 8 9 7 4 8 5 5 9 7 7 6 8 1 3 5 7 9 5 5 2 1 1 2 2 4 8 7 5 8 8 9 4\n",
            " 9 0]\n",
            "[6 9 3 7 2 1 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9\n",
            " 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4\n",
            " 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7\n",
            " 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 7 0 6 6 4 2 8 0 9 4 6 9 9 6 9\n",
            " 0 3 5 6 6 0 6 4 3 9 3 9 7 2 9 0 4 5 3 6 5 9 9 8 4 2 1 3 7 7 2 2 3 9 8 0 3\n",
            " 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7\n",
            " 4 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 8 8 5 5 1 6 2 8 8 9 9 7 6 2 2 2 3 4 8 8\n",
            " 3 6 0 9 7 7 0 1 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5\n",
            " 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8\n",
            " 4 5 8 7 9 8 5 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4 3 8 3 5 6 0 0 3 0 5 0 0 4 1\n",
            " 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 7 4 8 3 4\n",
            " 0 5 1 9 4 5 7 6 3 7 0 5 9 7 5 9 7 4 2 1 9 0 7 5 3 3 6 3 9 6 9 5 0 1 5 5 8\n",
            " 3 3 6 2 6 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC5w4I2YRUsr"
      },
      "source": [
        "Looks good. I'm done! Some places for improvement are at the end though. I should learn how to do everything without switching back to numpy to improve efficiency (not having to change back to numpy array)."
      ]
    }
  ]
}