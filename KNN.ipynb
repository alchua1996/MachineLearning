{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNN",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxjTc987KJkW"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16Q_YPyGKO2i"
      },
      "source": [
        "Today, we'll be going through a tutorial for the $K$ Nearest Neighbors (KNN) algorithm. In this tutorial, we'll run though an implementation of KNN using only numpy and a mode function from the statistics library. \n",
        "\n",
        "This will be a pretty barebones tutorial just to demonstrate the implementation of the algorithm for classification purposes. We can modify the algorithm for regression by taking the average, rather than the mode, of the K nearest neighbors. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A5U7ISXK0Le"
      },
      "source": [
        "#Theory of KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVCW1u7IK5A5"
      },
      "source": [
        "For nonlinear problems, we'll have to take a different approach from linear regression or linear SVM. One \"lazy\" way to approach this problem is KNN. The algorithm can be summarized as follows:\n",
        "\n",
        "1.   Pick a number of nearest neighbors, $K$.\n",
        "2.   Get all your data and put it all into one array.\n",
        "3.   For each data point, calculate the \"distance\" between each point using a specific norm. (e.g. Euclidean, Taxicab, and so on)\n",
        "4.   Sort all the distances from least to greatest.\n",
        "5.   Pick the smallest K distances (you can choose to include the distance of the datapoint to itself, but we excluded it).\n",
        "6.   For classification, take the mode of the label. For regression, take the average (or weighted average).\n",
        "\n",
        "This sounds very simple. Notice the lack of training and testing sets (hence the name \"lazy\"), and this is a very effective algorithm for general datasets since there are no assumptions about the data (other than labels) and no loss function. \n",
        "\n",
        "However, it's very likely that you'll have high variance for smaller datasets and small values of $K$ as well. Another isssue is the computation time. With many features, we'll have to find the norm a very large vector multiple times, so this dataset doesn't scale very well with dimension. \n",
        "\n",
        "Okay, with this out of the way, let's go through the implementation of this algorithm for a simple dataset, like this iris dataset since it comes with sklearn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZwxa1WvMYix"
      },
      "source": [
        "#Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMPEPrSZvhM0"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from statistics import mode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIUCAC26KON3"
      },
      "source": [
        "#Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq_j5ExBvkiZ"
      },
      "source": [
        "#using sklearn dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  \n",
        "y = iris.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28fHQcUjMnHk"
      },
      "source": [
        "#Load Data into One Array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2Mx5F8EMmA5"
      },
      "source": [
        "y_shape = y.reshape((y.shape[0],1)) #reshape to place in array without error \n",
        "data = np.hstack((X,y_shape)) #stick data in one array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxI6rPoNvqNV",
        "outputId": "ebfd2dfe-c607-4d61-e62e-236175e58ae0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5.1 3.5 0. ]\n",
            " [4.9 3.  0. ]\n",
            " [4.7 3.2 0. ]\n",
            " [4.6 3.1 0. ]\n",
            " [5.  3.6 0. ]\n",
            " [5.4 3.9 0. ]\n",
            " [4.6 3.4 0. ]\n",
            " [5.  3.4 0. ]\n",
            " [4.4 2.9 0. ]\n",
            " [4.9 3.1 0. ]\n",
            " [5.4 3.7 0. ]\n",
            " [4.8 3.4 0. ]\n",
            " [4.8 3.  0. ]\n",
            " [4.3 3.  0. ]\n",
            " [5.8 4.  0. ]\n",
            " [5.7 4.4 0. ]\n",
            " [5.4 3.9 0. ]\n",
            " [5.1 3.5 0. ]\n",
            " [5.7 3.8 0. ]\n",
            " [5.1 3.8 0. ]\n",
            " [5.4 3.4 0. ]\n",
            " [5.1 3.7 0. ]\n",
            " [4.6 3.6 0. ]\n",
            " [5.1 3.3 0. ]\n",
            " [4.8 3.4 0. ]\n",
            " [5.  3.  0. ]\n",
            " [5.  3.4 0. ]\n",
            " [5.2 3.5 0. ]\n",
            " [5.2 3.4 0. ]\n",
            " [4.7 3.2 0. ]\n",
            " [4.8 3.1 0. ]\n",
            " [5.4 3.4 0. ]\n",
            " [5.2 4.1 0. ]\n",
            " [5.5 4.2 0. ]\n",
            " [4.9 3.1 0. ]\n",
            " [5.  3.2 0. ]\n",
            " [5.5 3.5 0. ]\n",
            " [4.9 3.6 0. ]\n",
            " [4.4 3.  0. ]\n",
            " [5.1 3.4 0. ]\n",
            " [5.  3.5 0. ]\n",
            " [4.5 2.3 0. ]\n",
            " [4.4 3.2 0. ]\n",
            " [5.  3.5 0. ]\n",
            " [5.1 3.8 0. ]\n",
            " [4.8 3.  0. ]\n",
            " [5.1 3.8 0. ]\n",
            " [4.6 3.2 0. ]\n",
            " [5.3 3.7 0. ]\n",
            " [5.  3.3 0. ]\n",
            " [7.  3.2 1. ]\n",
            " [6.4 3.2 1. ]\n",
            " [6.9 3.1 1. ]\n",
            " [5.5 2.3 1. ]\n",
            " [6.5 2.8 1. ]\n",
            " [5.7 2.8 1. ]\n",
            " [6.3 3.3 1. ]\n",
            " [4.9 2.4 1. ]\n",
            " [6.6 2.9 1. ]\n",
            " [5.2 2.7 1. ]\n",
            " [5.  2.  1. ]\n",
            " [5.9 3.  1. ]\n",
            " [6.  2.2 1. ]\n",
            " [6.1 2.9 1. ]\n",
            " [5.6 2.9 1. ]\n",
            " [6.7 3.1 1. ]\n",
            " [5.6 3.  1. ]\n",
            " [5.8 2.7 1. ]\n",
            " [6.2 2.2 1. ]\n",
            " [5.6 2.5 1. ]\n",
            " [5.9 3.2 1. ]\n",
            " [6.1 2.8 1. ]\n",
            " [6.3 2.5 1. ]\n",
            " [6.1 2.8 1. ]\n",
            " [6.4 2.9 1. ]\n",
            " [6.6 3.  1. ]\n",
            " [6.8 2.8 1. ]\n",
            " [6.7 3.  1. ]\n",
            " [6.  2.9 1. ]\n",
            " [5.7 2.6 1. ]\n",
            " [5.5 2.4 1. ]\n",
            " [5.5 2.4 1. ]\n",
            " [5.8 2.7 1. ]\n",
            " [6.  2.7 1. ]\n",
            " [5.4 3.  1. ]\n",
            " [6.  3.4 1. ]\n",
            " [6.7 3.1 1. ]\n",
            " [6.3 2.3 1. ]\n",
            " [5.6 3.  1. ]\n",
            " [5.5 2.5 1. ]\n",
            " [5.5 2.6 1. ]\n",
            " [6.1 3.  1. ]\n",
            " [5.8 2.6 1. ]\n",
            " [5.  2.3 1. ]\n",
            " [5.6 2.7 1. ]\n",
            " [5.7 3.  1. ]\n",
            " [5.7 2.9 1. ]\n",
            " [6.2 2.9 1. ]\n",
            " [5.1 2.5 1. ]\n",
            " [5.7 2.8 1. ]\n",
            " [6.3 3.3 2. ]\n",
            " [5.8 2.7 2. ]\n",
            " [7.1 3.  2. ]\n",
            " [6.3 2.9 2. ]\n",
            " [6.5 3.  2. ]\n",
            " [7.6 3.  2. ]\n",
            " [4.9 2.5 2. ]\n",
            " [7.3 2.9 2. ]\n",
            " [6.7 2.5 2. ]\n",
            " [7.2 3.6 2. ]\n",
            " [6.5 3.2 2. ]\n",
            " [6.4 2.7 2. ]\n",
            " [6.8 3.  2. ]\n",
            " [5.7 2.5 2. ]\n",
            " [5.8 2.8 2. ]\n",
            " [6.4 3.2 2. ]\n",
            " [6.5 3.  2. ]\n",
            " [7.7 3.8 2. ]\n",
            " [7.7 2.6 2. ]\n",
            " [6.  2.2 2. ]\n",
            " [6.9 3.2 2. ]\n",
            " [5.6 2.8 2. ]\n",
            " [7.7 2.8 2. ]\n",
            " [6.3 2.7 2. ]\n",
            " [6.7 3.3 2. ]\n",
            " [7.2 3.2 2. ]\n",
            " [6.2 2.8 2. ]\n",
            " [6.1 3.  2. ]\n",
            " [6.4 2.8 2. ]\n",
            " [7.2 3.  2. ]\n",
            " [7.4 2.8 2. ]\n",
            " [7.9 3.8 2. ]\n",
            " [6.4 2.8 2. ]\n",
            " [6.3 2.8 2. ]\n",
            " [6.1 2.6 2. ]\n",
            " [7.7 3.  2. ]\n",
            " [6.3 3.4 2. ]\n",
            " [6.4 3.1 2. ]\n",
            " [6.  3.  2. ]\n",
            " [6.9 3.1 2. ]\n",
            " [6.7 3.1 2. ]\n",
            " [6.9 3.1 2. ]\n",
            " [5.8 2.7 2. ]\n",
            " [6.8 3.2 2. ]\n",
            " [6.7 3.3 2. ]\n",
            " [6.7 3.  2. ]\n",
            " [6.3 2.5 2. ]\n",
            " [6.5 3.  2. ]\n",
            " [6.2 3.4 2. ]\n",
            " [5.9 3.  2. ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZf4PJYoM-e1"
      },
      "source": [
        "#Implementing the Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP8YZp3SNScc"
      },
      "source": [
        "Okay, so first, let's start by choosing a value of $K$. In this case, we chose $5$ for no reason. We'll also initialize a vector where we can store our predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDoTZRZQNh_o"
      },
      "source": [
        "K = 5\n",
        "y_pred = np.zeros(data.shape[0], dtype = 'float')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nSZlxUoOm9i"
      },
      "source": [
        "Now, let's move onto the main algorithm. First, we'll initialize an outer loop, which goes over each data point. The neighbors variable will save the output of the $K$ nearest neighbors and the distance between each piont. \n",
        "\n",
        "In the first inner loop, we find the distance between points. In the second inner loop, we pick the neighbors after argsorting the data. Once we take the mode, we'll save it as our prediction and repeat. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBwhwdFWvyPF"
      },
      "source": [
        "distance = np.zeros(data.shape[0], dtype = 'float') #distance between points\n",
        "neighbors = np.zeros(K, dtype = 'float') #save neighbors \"y\" value\n",
        "for n in range(data.shape[0]): #loop over each datapoint\n",
        "    for m in range(data.shape[0]):  \n",
        "        distance[m] = np.linalg.norm(data[n,:]-data[m,:]) #euclidean norm of points\n",
        "    arg_sort = np.argsort(distance) #stores indexes of distance array from smallest distance to largest\n",
        "    for k in range(K):\n",
        "        neighbors[k] = data[arg_sort[k+1], data.shape[1] - 1] #pick neighbors\n",
        "    y_pred[n] = mode(neighbors) #predict by taking mode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI_eH34dNoKg",
        "outputId": "72e63323-967d-4d8e-9d19-2ba68860f61f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "#check output\n",
        "print(y_pred) \n",
        "print(data[:,data.shape[1] - 1]) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EPV5WB7NspF"
      },
      "source": [
        "Since this dataset was very simple, we got perfect (?) accuracy, but this was expected anyway. We'll use this in a harder dataset eventually, like fraud detection."
      ]
    }
  ]
}