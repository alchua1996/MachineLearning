{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "## Import libraries"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd",
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Define Functions"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def read_dataset(feature_file, label_file):\n    ''' Read data set in *.csv to data frame in Pandas'''\n    df_X = pd.read_csv(feature_file)\n    df_y = pd.read_csv(label_file)\n    X = df_X.values  # convert values in dataframe to numpy array (features)\n    y = df_y.values  # convert values in dataframe to numpy array (label)\n    return X, y\n\ndef RMSE(ypred, yexact):\n    return np.sqrt(np.sum((ypred - yexact)**2)/ ypred.shape[0])\n\ndef PCC(y_pred, y_test):\n    from scipy import stats\n    a = y_test\n    b = y_pred\n    accuracy = stats.pearsonr(a, b)\n    return accuracy",
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class decisiontrees():\n    def __init__(self, max_depth=5, current_depth=1,mode = 'Classification'):        \n        ''' initalize the decision trees parameters '''\n        self.max_depth = max_depth\n        self.current_depth = current_depth\n        self.left_tree = None\n        self.right_tree = None\n        self.mode = mode\n        \n    def fit(self, X, y):\n        self.X = X\n        self.y = y        \n        self.n_features = X.shape[1]\n        self.n_samples = X.shape[0]\n        if self.current_depth <= self.max_depth:\n#             print('Current depth = %d' % self.current_depth)\n            if self.mode == 'Classification':\n                self.GINI = self.GINI_calculation(self.y)\n                self.best_feature_id, self.best_gain, self.best_split_value = self.find_best_split()\n                if self.best_gain > 0:\n                    self.split_trees()\n            if self.mode == 'Regression': # Homework\n                self.SD = np.std(self.y)\n                self.best_feature_id, self.best_score, self.best_split_value = self.find_best_split()\n                if self.SD > 0:\n                    self.split_trees()\n        \n    def predict(self, X_test): # predict all the samples in the test set by tree propagation for each sample\n        n_test = X_test.shape[0]\n        ypred = np.zeros(n_test, dtype=int)  \n        for i in range(n_test):\n            ypred[i] = self.tree_propogation(X_test[i])            \n        return ypred\n                \n    def tree_propogation(self, feature): # goes through the tree to predict one sample\n        if self.is_leaf_node(): # if the node is a leaf, we predict the label\n            return self.predict_label()\n        if feature[self.best_feature_id] < self.best_split_value:  # choose which path to take based on best split value\n            child_tree = self.left_tree\n        else:\n            child_tree = self.right_tree\n        return child_tree.tree_propogation(feature) # recursively call the tree propagation\n    \n    def predict_label(self): # predicts the label in a leaf node\n        if self.mode == 'Classification':\n            unique, counts = np.unique(self.y, return_counts=True) # find the labels in the leaf node and their counts\n            label = None  # initialize label and largest count\n            max_count = 0\n            for i in range(unique.size):  # go through the counts of each label\n                if counts[i] > max_count: # find the label that has the most samples associated with it\n                    max_count = counts[i]\n                    label = unique[i]\n            return label\n        if self.mode == 'Regression':\n            label = np.average(self.y)\n            return label\n    \n    def is_leaf_node(self):\n        return self.left_tree is None\n    \n    def split_trees(self):\n        # create a left tree\n        self.left_tree = decisiontrees(max_depth = self.max_depth, current_depth = self.current_depth + 1, mode = self.mode)\n        self.right_tree = decisiontrees(max_depth = self.max_depth, current_depth = self.current_depth + 1, mode = self.mode)\n        best_feature_values = self.X[:, self.best_feature_id]\n        left_indices = np.where(best_feature_values < self.best_split_value)\n        right_indices = np.where(best_feature_values >= self.best_split_value)\n        left_tree_X = self.X[left_indices]\n        left_tree_y = self.y[left_indices]\n        right_tree_X = self.X[right_indices]\n        right_tree_y = self.y[right_indices]\n        \n        # fit left and right tree\n        self.left_tree.fit(left_tree_X, left_tree_y)\n        self.right_tree.fit(right_tree_X, right_tree_y)\n        \n    \n    def find_best_split(self):\n        best_feature_id = None\n        best_gain = 0\n        best_split_value = None\n        for feature_id in range(self.n_features):\n            current_gain, current_split_value = self.find_best_split_one_feature(feature_id)\n            if current_gain is None:\n                continue\n            if best_gain < current_gain:\n                best_feature_id = feature_id\n                best_gain = current_gain                \n                best_split_value = current_split_value\n        return best_feature_id, best_gain, best_split_value\n    \n    def find_best_split_one_feature(self, feature_id):\n        '''\n            Return information_gain, split_value\n        '''\n        feature_values = self.X[:, feature_id]\n        unique_feature_values = np.unique(feature_values)\n        best_gain = 0.0\n        best_split_value = None\n        if len(unique_feature_values) == 1:\n            return best_gain, best_split_value\n        for fea_val in unique_feature_values:\n            left_indices = np.where(feature_values < fea_val)\n            right_indices = np.where(feature_values >= fea_val)\n            left_tree_X = self.X[left_indices]\n            left_tree_y = self.y[left_indices]\n\n            right_tree_X = self.X[right_indices]\n            right_tree_y = self.y[right_indices]\n            left_GINI = self.GINI_calculation(left_tree_y)\n            right_GINI = self.GINI_calculation(right_tree_y)\n            left_score = np.std(left_tree_y)\n            right_score = np.std(right_tree_y)\n\n            # calculate gain\n            if self.mode == 'Classification':\n                left_n_samples = left_tree_X.shape[0]\n                right_n_samples = right_tree_X.shape[0]\n                current_gain = self.GINI - (left_n_samples/self.n_samples * left_GINI + right_n_samples/self.n_samples * right_GINI)\n            if self.mode == 'Regression':\n                left_n_samples = left_tree_X.shape[0]\n                right_n_samples = right_tree_X.shape[0]\n                current_gain = self.SD - (left_n_samples/self.n_samples * left_score + right_n_samples/self.n_samples * right_score)\n            if best_gain < current_gain:\n                best_gain = current_gain\n                best_split_value = fea_val\n        return best_gain, best_split_value                                    \n            \n    def GINI_calculation(self, y):\n        if y.size == 0 or y is None:\n            return 0.0\n        unique, counts = np.unique(y, return_counts=True)\n        prob = counts/y.size\n        return 1.0 - np.sum(prob*prob)",
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# ===================================Regression==================================\n# Homework\nX_train, y_train = read_dataset('airfoil_self_noise_X_train.csv', 'airfoil_self_noise_y_train.csv')\nX_test, y_test = read_dataset('airfoil_self_noise_X_test.csv', 'airfoil_self_noise_y_test.csv')\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\nmydt = decisiontrees(max_depth=11, mode='Regression')\nmydt.fit(X_train, y_train)\ny_pred = mydt.predict(X_test)\nprint('PCC of our model ', PCC(y_pred, y_test.ravel()))\nprint('RSME of our model ', RMSE(y_pred, y_test.ravel()))",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(1202, 5)\n(301, 5)\n(1202, 1)\n(301, 1)\nPCC of our model  (0.9110566947980279, 4.4333587043288576e-117)\nRSME of our model  2.829249949323586\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}