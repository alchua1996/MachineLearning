{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(feature_file, label_file):\n",
    "    ''' Read data set in *.csv to data frame in Pandas'''\n",
    "    df_X = pd.read_csv(feature_file)\n",
    "    df_y = pd.read_csv(label_file)\n",
    "    X = df_X.values \n",
    "    y = df_y.values \n",
    "    return X, y\n",
    "\n",
    "def normalize_features(X_train, X_test):\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "    scaler = StandardScaler() \n",
    "    scaler.fit(X_train) \n",
    "    X_train_norm = scaler.transform(X_train) \n",
    "    X_test_norm = scaler.transform(X_test) \n",
    "    return X_train_norm, X_test_norm\n",
    "\n",
    "def one_hot_encoder(y_train, y_test):\n",
    "    ''' convert label to a vector under one-hot-code fashion '''\n",
    "    from sklearn import preprocessing\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(y_train)\n",
    "    y_train_ohe = lb.transform(y_train)\n",
    "    y_test_ohe = lb.transform(y_test)\n",
    "    return y_train_ohe, y_test_ohe\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def softmax(z):\n",
    "    exp_value = np.exp(z-np.amax(z, axis=1, keepdims=True))\n",
    "    softmax_scores = exp_value / np.sum(exp_value, axis=1, keepdims=True)\n",
    "    return softmax_scores\n",
    "\n",
    "def accuracy(ypred, yexact):\n",
    "    p = np.array(ypred == yexact, dtype = int)\n",
    "    return np.sum(p)/float(len(yexact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, X, y, layer1=100, layer2=100, lr=0.01):\n",
    "        self.X = X \n",
    "        self.y = y \n",
    "        self.layer1 = layer1 \n",
    "        self.layer2 = layer2 \n",
    "        self.lr = lr \n",
    "        self.nn = X.shape[1] \n",
    "        self.W1 = np.random.randn(self.nn, self.layer1) / np.sqrt(self.nn)\n",
    "        self.b1 = np.zeros((1, self.layer1)) \n",
    "        self.output = y.shape[1]\n",
    "        self.W2 = np.random.randn(self.layer1, self.layer2) / np.sqrt(self.layer1)\n",
    "        self.b2 = np.zeros((1, self.layer2))\n",
    "        self.W3 = np.random.randn(self.layer2, self.output) / np.sqrt(self.layer2)\n",
    "        self.b3 = np.zeros((1, self.output))\n",
    "           \n",
    "    def feed_forward(self):\n",
    "        self.z1 = np.dot(self.X, self.W1) + self.b1\n",
    "        #self.f1 = np.tanh(self.z1)\n",
    "        self.f1 = sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.f1, self.W2) + self.b2 \n",
    "        #self.f2 = np.tanh(self.z2)\n",
    "        self.f2 = sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.f2, self.W3) + self.b3\n",
    "        self.y_hat = softmax(self.z3)\n",
    "        \n",
    "    def back_propagation(self):\n",
    "        d3 = self.y_hat - self.y\n",
    "        dW3 = np.dot(self.f2.T, d3)\n",
    "        db3 = np.sum(d3, axis=0, keepdims=True)\n",
    "        d2 = np.dot(d3,self.W3.T)*self.f2*(1 - self.f2)\n",
    "        dW2 = np.dot(self.f1.T, d2)\n",
    "        db2 = np.sum(d2, axis=0, keepdims=True)\n",
    "        d1 = np.dot(d2, self.W2.T) * self.f1* (1 - self.f1)\n",
    "        dW1 = np.dot((self.X).T, d1) \n",
    "        db1 = np.sum(d1, axis=0, keepdims=True)\n",
    "        \n",
    "        self.W1 = self.W1 - self.lr * dW1\n",
    "        self.b1 = self.b1 - self.lr * db1\n",
    "        self.W2 = self.W2 - self.lr * dW2\n",
    "        self.b2 = self.b2 - self.lr * db2\n",
    "        self.W3 = self.W3 - self.lr * dW3\n",
    "        self.b3 = self.b3 - self.lr * db3\n",
    "        \n",
    "    def cross_entropy_loss(self):\n",
    "        self.feed_forward()\n",
    "        self.loss = -np.sum(self.y*np.log(self.y_hat + 1e-6))\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        z1 = np.dot(X_test, self.W1) + self.b1\n",
    "        f1 = sigmoid(z1)\n",
    "        z2 = np.dot(f1, self.W2) + self.b2\n",
    "        f2 = sigmoid(z2)\n",
    "        z3 = np.dot(f2, self.W3) + self.b3\n",
    "        y_hat_test = softmax(z3)\n",
    "        labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        num_test_samples = X_test.shape[0]\n",
    "        ypred = np.zeros(num_test_samples, dtype=int) \n",
    "        for i in range(num_test_samples):\n",
    "            ypred[i] = labels[np.argmax(y_hat_test[i,:])]\n",
    "        return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 20, current loss = 6061.61994\n",
      "epoch = 40, current loss = 4549.82615\n",
      "epoch = 60, current loss = 3775.27018\n",
      "epoch = 80, current loss = 8046.30391\n",
      "epoch = 100, current loss = 4591.64461\n",
      "epoch = 120, current loss = 2671.75361\n",
      "epoch = 140, current loss = 2632.10837\n",
      "epoch = 160, current loss = 2525.22104\n",
      "epoch = 180, current loss = 2632.67727\n",
      "epoch = 200, current loss = 2375.13687\n",
      "epoch = 220, current loss = 2510.53374\n",
      "epoch = 240, current loss = 2447.49676\n",
      "epoch = 260, current loss = 2384.02595\n",
      "epoch = 280, current loss = 2393.59023\n",
      "epoch = 300, current loss = 2093.39354\n",
      "epoch = 320, current loss = 1484.43318\n",
      "epoch = 340, current loss = 1334.79918\n",
      "epoch = 360, current loss = 1280.36206\n",
      "epoch = 380, current loss = 867.40875\n",
      "epoch = 400, current loss = 328.95605\n",
      "epoch = 420, current loss = 258.20601\n",
      "epoch = 440, current loss = 223.79846\n",
      "epoch = 460, current loss = 208.91001\n",
      "epoch = 480, current loss = 200.89485\n",
      "epoch = 500, current loss = 194.97090\n",
      "epoch = 520, current loss = 185.38898\n",
      "epoch = 540, current loss = 172.72587\n",
      "epoch = 560, current loss = 356.79999\n",
      "epoch = 580, current loss = 301.47315\n",
      "epoch = 600, current loss = 257.20578\n",
      "epoch = 620, current loss = 228.47393\n",
      "epoch = 640, current loss = 219.64128\n",
      "epoch = 660, current loss = 210.12187\n",
      "epoch = 680, current loss = 203.56126\n",
      "epoch = 700, current loss = 195.14371\n",
      "epoch = 720, current loss = 181.94490\n",
      "epoch = 740, current loss = 167.55052\n",
      "epoch = 760, current loss = 158.45070\n",
      "epoch = 780, current loss = 150.35195\n",
      "epoch = 800, current loss = 143.02815\n",
      "epoch = 820, current loss = 127.47264\n",
      "epoch = 840, current loss = 121.56917\n",
      "epoch = 860, current loss = 114.28815\n",
      "epoch = 880, current loss = 106.30387\n",
      "epoch = 900, current loss = 94.79941\n",
      "epoch = 920, current loss = 90.35530\n",
      "epoch = 940, current loss = 87.57134\n",
      "epoch = 960, current loss = 85.40646\n",
      "epoch = 980, current loss = 84.32447\n",
      "epoch = 1000, current loss = 79.37021\n",
      "Accuracy of our model  0.85\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = read_dataset('MNIST_X_train.csv', 'MNIST_y_train.csv')\n",
    "X_test, y_test = read_dataset('MNIST_X_test.csv', 'MNIST_y_test.csv')\n",
    "X_train_norm, X_test_norm = normalize_features(X_train, X_test)\n",
    "y_train_ohe, y_test_ohe = one_hot_encoder(y_train, y_test)\n",
    "myNN = NN(X_train_norm, y_train_ohe, 100, 100, lr=0.01)  \n",
    "epoch_num = 1000\n",
    "for i in range(epoch_num):\n",
    "    myNN.feed_forward()\n",
    "    myNN.back_propagation()\n",
    "    myNN.cross_entropy_loss()\n",
    "    if ((i+1)%20 == 0):\n",
    "        print('epoch = %d, current loss = %.5f' % (i+1, myNN.loss))         \n",
    "        \n",
    "y_pred = myNN.predict(X_test_norm)\n",
    "print('Accuracy of our model ', accuracy(y_pred, y_test.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
